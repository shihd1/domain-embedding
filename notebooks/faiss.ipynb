{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (0.2.7)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.8.0.post1-cp311-cp311-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.1.16-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.7.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain-community) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain-community) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.7 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain-community) (0.2.7)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.12 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain-community) (0.2.12)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain-community) (0.1.85)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain-community) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain-community) (8.2.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Collecting langchain-core<0.3.0,>=0.2.12 (from langchain-community)\n",
      "  Downloading langchain_core-0.2.20-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.32.0 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain-openai) (1.35.13)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain<0.3.0,>=0.2.7->langchain-community) (0.2.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain<0.3.0,>=0.2.7->langchain-community) (2.8.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.12->langchain-community) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain-community) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.7->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.7->langchain-community) (2.20.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.32.0->langchain-openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\clkds12\\appdata\\local\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Using cached faiss_cpu-1.8.0.post1-cp311-cp311-win_amd64.whl (14.6 MB)\n",
      "Downloading langchain_openai-0.1.16-py3-none-any.whl (46 kB)\n",
      "   ---------------------------------------- 0.0/46.1 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 20.5/46.1 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 20.5/46.1 kB ? eta -:--:--\n",
      "   ----------------------------------- ---- 41.0/46.1 kB 245.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 46.1/46.1 kB 256.0 kB/s eta 0:00:00\n",
      "Using cached tiktoken-0.7.0-cp311-cp311-win_amd64.whl (799 kB)\n",
      "Downloading langchain_core-0.2.20-py3-none-any.whl (371 kB)\n",
      "   ---------------------------------------- 0.0/371.7 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/371.7 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 51.2/371.7 kB 650.2 kB/s eta 0:00:01\n",
      "   -------- ------------------------------ 81.9/371.7 kB 651.6 kB/s eta 0:00:01\n",
      "   ----------- -------------------------- 112.6/371.7 kB 652.2 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 143.4/371.7 kB 655.8 kB/s eta 0:00:01\n",
      "   ---------------- --------------------- 163.8/371.7 kB 653.6 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 194.6/371.7 kB 653.6 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 225.3/371.7 kB 655.6 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 276.5/371.7 kB 680.9 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 297.0/371.7 kB 654.6 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 348.2/371.7 kB 696.7 kB/s eta 0:00:01\n",
      "   -------------------------------------  368.6/371.7 kB 673.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- 371.7/371.7 kB 660.8 kB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu, tiktoken, langchain-core, langchain-openai\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.2.12\n",
      "    Uninstalling langchain-core-0.2.12:\n",
      "      Successfully uninstalled langchain-core-0.2.12\n",
      "Successfully installed faiss-cpu-1.8.0.post1 langchain-core-0.2.20 langchain-openai-0.1.16 tiktoken-0.7.0\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain-community faiss-cpu langchain-openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "endpoint = os.environ.get(\"OPENAI_URL\")\n",
    "api_key = os.environ.get(\"OPEN_AI_KEY\")\n",
    "deployment = os.environ.get(\"OPENAI_DEPLOY\")\n",
    "\n",
    "client = openai.AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=api_key,\n",
    "    api_version=\"2024-02-01\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from pdfminer.high_level import extract_text\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\"\"\"\n",
    "Function for document processing.\n",
    "Initially the document is read using the extract_text function, as a result we have a reasonable processing\n",
    "but with noise and loss of context.\n",
    "NLTK is used to count tokens per text and thus generate document partitions.\n",
    "The output is obtained with the execute function and returns three results:\n",
    " - Previous: Part before the current one\n",
    " - Current: Current part\n",
    " - Next: Part after the current one\n",
    "\n",
    " These outputs are used with the help of LLMs to optimize the extracted text by combining the generated parts. As a consequence, \n",
    " Some information may be repeated, however, it will not be lost.\n",
    "\"\"\"\n",
    "\n",
    "class DocProcessing:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def filter_lines(self, text):\n",
    "        filtered_lines = []\n",
    "        for line in text.splitlines():\n",
    "            stripped_line = line.strip()\n",
    "            if len(stripped_line) >= 15 and len(re.findall(r'[a-zA-Z]', stripped_line)) >= 7:\n",
    "                filtered_lines.append(stripped_line)\n",
    "        return \"\\n\".join(filtered_lines)\n",
    "\n",
    "    def tokenize_text(self, text):\n",
    "        return word_tokenize(text)\n",
    "\n",
    "    def split_into_parts(self, tokens, current_part_size=700, context_size=250):\n",
    "        parts = []\n",
    "        total_tokens = len(tokens)\n",
    "        index = 0\n",
    "        \n",
    "        while index < total_tokens:\n",
    "            current_end = min(index + current_part_size, total_tokens)\n",
    "            previous_start = max(index - context_size, 0)\n",
    "            next_end = min(current_end + context_size, total_tokens)\n",
    "            \n",
    "            previous = tokens[previous_start:index]\n",
    "            actual = tokens[index:current_end]\n",
    "            next = tokens[current_end:next_end]\n",
    "            \n",
    "            parts.append({\n",
    "                'previous': previous,\n",
    "                'actual': actual,\n",
    "                'next': next\n",
    "            })\n",
    "            \n",
    "            index = current_end\n",
    "        \n",
    "        return parts\n",
    "\n",
    "    def execute(self):\n",
    "        try:\n",
    "            extracted_text = extract_text(self.filename)\n",
    "\n",
    "            filtered_text = self.filter_lines(extracted_text)\n",
    "\n",
    "            tokens = self.tokenize_text(filtered_text)\n",
    "\n",
    "            parts = self.split_into_parts(tokens)\n",
    "\n",
    "            # return[' '.join(part['actual']) for part in parts]\n",
    "            parts_dict = {\n",
    "                f\"part {i+1}\": {\n",
    "                    \"previous\": ' '.join(part['previous']),\n",
    "                    \"actual\": ' '.join(part['actual']),\n",
    "                    \"next\": ' '.join(part['next'])\n",
    "                } for i, part in enumerate(parts)\n",
    "            }\n",
    "            return parts_dict\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "processor = DocProcessing(filename=\"../files/Forsthoffer's Vol 1 - Rotating Equipment.pdf\")\n",
    "docs = processor.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.67 %!\n",
      "Progress: 1.34 %!\n",
      "Progress: 2.01 %!\n",
      "Progress: 2.68 %!\n",
      "Progress: 3.36 %!\n",
      "Progress: 4.03 %!\n",
      "Progress: 4.70 %!\n",
      "Progress: 5.37 %!\n",
      "Progress: 6.04 %!\n",
      "Progress: 6.71 %!\n",
      "Progress: 7.38 %!\n",
      "Progress: 8.05 %!\n",
      "Progress: 8.72 %!\n",
      "Progress: 9.40 %!\n",
      "Progress: 10.07 %!\n",
      "Progress: 10.74 %!\n",
      "Progress: 11.41 %!\n",
      "Progress: 12.08 %!\n",
      "Progress: 12.75 %!\n",
      "Progress: 13.42 %!\n",
      "Progress: 14.09 %!\n",
      "Progress: 14.77 %!\n",
      "Progress: 15.44 %!\n",
      "Progress: 16.11 %!\n",
      "Progress: 16.78 %!\n",
      "Progress: 17.45 %!\n",
      "Progress: 18.12 %!\n",
      "Progress: 18.79 %!\n",
      "Progress: 19.46 %!\n",
      "Progress: 20.13 %!\n",
      "Progress: 20.81 %!\n",
      "Progress: 21.48 %!\n",
      "Progress: 22.15 %!\n",
      "Progress: 22.82 %!\n",
      "Progress: 23.49 %!\n",
      "Progress: 24.16 %!\n",
      "Progress: 24.83 %!\n",
      "Progress: 25.50 %!\n",
      "Progress: 26.17 %!\n",
      "Progress: 26.85 %!\n",
      "Progress: 27.52 %!\n",
      "Progress: 28.19 %!\n",
      "Progress: 28.86 %!\n",
      "Progress: 29.53 %!\n",
      "Progress: 30.20 %!\n",
      "Progress: 30.87 %!\n",
      "Progress: 31.54 %!\n",
      "Progress: 32.21 %!\n",
      "Progress: 32.89 %!\n",
      "Progress: 33.56 %!\n",
      "Progress: 34.23 %!\n",
      "Progress: 34.90 %!\n",
      "Progress: 35.57 %!\n",
      "Progress: 36.24 %!\n",
      "Progress: 36.91 %!\n",
      "Progress: 37.58 %!\n",
      "Progress: 38.26 %!\n",
      "Progress: 38.93 %!\n",
      "Progress: 39.60 %!\n",
      "Progress: 40.27 %!\n",
      "Progress: 40.94 %!\n",
      "Progress: 41.61 %!\n",
      "Progress: 42.28 %!\n",
      "Progress: 42.95 %!\n",
      "Progress: 43.62 %!\n",
      "Progress: 44.30 %!\n",
      "Progress: 44.97 %!\n",
      "Progress: 45.64 %!\n",
      "Progress: 46.31 %!\n",
      "Progress: 46.98 %!\n",
      "Progress: 47.65 %!\n",
      "Progress: 48.32 %!\n",
      "Progress: 48.99 %!\n",
      "Progress: 49.66 %!\n",
      "Progress: 50.34 %!\n",
      "Progress: 51.01 %!\n",
      "Progress: 51.68 %!\n",
      "Progress: 52.35 %!\n",
      "Progress: 53.02 %!\n",
      "Progress: 53.69 %!\n",
      "Progress: 54.36 %!\n",
      "Progress: 55.03 %!\n",
      "Progress: 55.70 %!\n",
      "Progress: 56.38 %!\n",
      "Progress: 57.05 %!\n",
      "Progress: 57.72 %!\n",
      "Progress: 58.39 %!\n",
      "Progress: 59.06 %!\n",
      "Progress: 59.73 %!\n",
      "Progress: 60.40 %!\n",
      "Progress: 61.07 %!\n",
      "Progress: 61.74 %!\n",
      "Progress: 62.42 %!\n",
      "Progress: 63.09 %!\n",
      "Progress: 63.76 %!\n",
      "Progress: 64.43 %!\n",
      "Progress: 65.10 %!\n",
      "Progress: 65.77 %!\n",
      "Progress: 66.44 %!\n",
      "Progress: 67.11 %!\n",
      "Progress: 67.79 %!\n",
      "Progress: 68.46 %!\n",
      "Progress: 69.13 %!\n",
      "Progress: 69.80 %!\n",
      "Progress: 70.47 %!\n",
      "Progress: 71.14 %!\n",
      "Progress: 71.81 %!\n",
      "Progress: 72.48 %!\n",
      "Progress: 73.15 %!\n",
      "Progress: 73.83 %!\n",
      "Progress: 74.50 %!\n",
      "Progress: 75.17 %!\n",
      "Progress: 75.84 %!\n",
      "Progress: 76.51 %!\n",
      "Progress: 77.18 %!\n",
      "Progress: 77.85 %!\n",
      "Progress: 78.52 %!\n",
      "Progress: 79.19 %!\n",
      "Progress: 79.87 %!\n",
      "Progress: 80.54 %!\n",
      "Progress: 81.21 %!\n",
      "Progress: 81.88 %!\n",
      "Progress: 82.55 %!\n",
      "Progress: 83.22 %!\n",
      "Progress: 83.89 %!\n",
      "Progress: 84.56 %!\n",
      "Progress: 85.23 %!\n",
      "Progress: 85.91 %!\n",
      "Progress: 86.58 %!\n",
      "Progress: 87.25 %!\n",
      "Progress: 87.92 %!\n",
      "Progress: 88.59 %!\n",
      "Progress: 89.26 %!\n",
      "Progress: 89.93 %!\n",
      "Progress: 90.60 %!\n",
      "Progress: 91.28 %!\n",
      "Progress: 91.95 %!\n",
      "Progress: 92.62 %!\n",
      "Progress: 93.29 %!\n",
      "Progress: 93.96 %!\n",
      "Progress: 94.63 %!\n",
      "Progress: 95.30 %!\n",
      "Progress: 95.97 %!\n",
      "Progress: 96.64 %!\n",
      "Progress: 97.32 %!\n",
      "Progress: 97.99 %!\n",
      "Progress: 98.66 %!\n",
      "Progress: 99.33 %!\n",
      "Progress: 100.00 %!\n",
      "============= STARTED! =============\n",
      "|       Working on Chunk 0 / 149!\n",
      "|       Working on Chunk 1 / 149!\n",
      "|       Working on Chunk 2 / 149!\n",
      "|       Working on Chunk 3 / 149!\n",
      "|       Working on Chunk 4 / 149!\n",
      "|       Working on Chunk 5 / 149!\n",
      "|       Working on Chunk 6 / 149!\n",
      "|       Working on Chunk 7 / 149!\n",
      "|       Working on Chunk 8 / 149!\n",
      "|       Working on Chunk 9 / 149!\n",
      "|       Working on Chunk 10 / 149!\n",
      "|       Working on Chunk 11 / 149!\n",
      "|       Working on Chunk 12 / 149!\n",
      "|       Working on Chunk 13 / 149!\n",
      "|       Working on Chunk 14 / 149!\n",
      "|       Working on Chunk 15 / 149!\n",
      "|       Working on Chunk 16 / 149!\n",
      "|       Working on Chunk 17 / 149!\n",
      "|       Working on Chunk 18 / 149!\n",
      "|       Working on Chunk 19 / 149!\n",
      "|       Working on Chunk 20 / 149!\n",
      "|       Working on Chunk 21 / 149!\n",
      "|       Working on Chunk 22 / 149!\n",
      "|       Working on Chunk 23 / 149!\n",
      "|       Working on Chunk 24 / 149!\n",
      "|       Working on Chunk 25 / 149!\n",
      "|       Working on Chunk 26 / 149!\n",
      "|       Working on Chunk 27 / 149!\n",
      "|       Working on Chunk 28 / 149!\n",
      "|       Working on Chunk 29 / 149!\n",
      "|       Working on Chunk 30 / 149!\n",
      "|       Working on Chunk 31 / 149!\n",
      "|       Working on Chunk 32 / 149!\n",
      "|       Working on Chunk 33 / 149!\n",
      "|       Working on Chunk 34 / 149!\n",
      "|       Working on Chunk 35 / 149!\n",
      "|       Working on Chunk 36 / 149!\n",
      "|       Working on Chunk 37 / 149!\n",
      "|       Working on Chunk 38 / 149!\n",
      "|       Working on Chunk 39 / 149!\n",
      "|       Working on Chunk 40 / 149!\n",
      "|       Working on Chunk 41 / 149!\n",
      "|       Working on Chunk 42 / 149!\n",
      "|       Working on Chunk 43 / 149!\n",
      "|       Working on Chunk 44 / 149!\n",
      "|       Working on Chunk 45 / 149!\n",
      "|       Working on Chunk 46 / 149!\n",
      "|       Working on Chunk 47 / 149!\n",
      "|       Working on Chunk 48 / 149!\n",
      "|       Working on Chunk 49 / 149!\n",
      "|       Working on Chunk 50 / 149!\n",
      "|       Working on Chunk 51 / 149!\n",
      "|       Working on Chunk 52 / 149!\n",
      "|       Working on Chunk 53 / 149!\n",
      "|       Working on Chunk 54 / 149!\n",
      "|       Working on Chunk 55 / 149!\n",
      "|       Working on Chunk 56 / 149!\n",
      "|       Working on Chunk 57 / 149!\n",
      "|       Working on Chunk 58 / 149!\n",
      "|       Working on Chunk 59 / 149!\n",
      "|       Working on Chunk 60 / 149!\n",
      "|       Working on Chunk 61 / 149!\n",
      "|       Working on Chunk 62 / 149!\n",
      "|       Working on Chunk 63 / 149!\n",
      "|       Working on Chunk 64 / 149!\n",
      "|       Working on Chunk 65 / 149!\n",
      "|       Working on Chunk 66 / 149!\n",
      "|       Working on Chunk 67 / 149!\n",
      "|       Working on Chunk 68 / 149!\n",
      "|       Working on Chunk 69 / 149!\n",
      "|       Working on Chunk 70 / 149!\n",
      "|       Working on Chunk 71 / 149!\n",
      "|       Working on Chunk 72 / 149!\n",
      "|       Working on Chunk 73 / 149!\n",
      "|       Working on Chunk 74 / 149!\n",
      "|       Working on Chunk 75 / 149!\n",
      "|       Working on Chunk 76 / 149!\n",
      "|       Working on Chunk 77 / 149!\n",
      "|       Working on Chunk 78 / 149!\n",
      "|       Working on Chunk 79 / 149!\n",
      "|       Working on Chunk 80 / 149!\n",
      "|       Working on Chunk 81 / 149!\n",
      "|       Working on Chunk 82 / 149!\n",
      "|       Working on Chunk 83 / 149!\n",
      "|       Working on Chunk 84 / 149!\n",
      "|       Working on Chunk 85 / 149!\n",
      "|       Working on Chunk 86 / 149!\n",
      "|       Working on Chunk 87 / 149!\n",
      "|       Working on Chunk 88 / 149!\n",
      "|       Working on Chunk 89 / 149!\n",
      "|       Working on Chunk 90 / 149!\n",
      "|       Working on Chunk 91 / 149!\n",
      "|       Working on Chunk 92 / 149!\n",
      "|       Working on Chunk 93 / 149!\n",
      "|       Working on Chunk 94 / 149!\n",
      "|       Working on Chunk 95 / 149!\n",
      "|       Working on Chunk 96 / 149!\n",
      "|       Working on Chunk 97 / 149!\n",
      "|       Working on Chunk 98 / 149!\n",
      "|       Working on Chunk 99 / 149!\n",
      "|       Working on Chunk 100 / 149!\n",
      "|       Working on Chunk 101 / 149!\n",
      "|       Working on Chunk 102 / 149!\n",
      "|       Working on Chunk 103 / 149!\n",
      "|       Working on Chunk 104 / 149!\n",
      "|       Working on Chunk 105 / 149!\n",
      "|       Working on Chunk 106 / 149!\n",
      "|       Working on Chunk 107 / 149!\n",
      "|       Working on Chunk 108 / 149!\n",
      "|       Working on Chunk 109 / 149!\n",
      "|       Working on Chunk 110 / 149!\n",
      "|       Working on Chunk 111 / 149!\n",
      "|       Working on Chunk 112 / 149!\n",
      "|       Working on Chunk 113 / 149!\n",
      "|       Working on Chunk 114 / 149!\n",
      "|       Working on Chunk 115 / 149!\n",
      "|       Working on Chunk 116 / 149!\n",
      "|       Working on Chunk 117 / 149!\n",
      "|       Working on Chunk 118 / 149!\n",
      "|       Working on Chunk 119 / 149!\n",
      "|       Working on Chunk 120 / 149!\n",
      "|       Working on Chunk 121 / 149!\n",
      "|       Working on Chunk 122 / 149!\n",
      "|       Working on Chunk 123 / 149!\n",
      "|       Working on Chunk 124 / 149!\n",
      "|       Working on Chunk 125 / 149!\n",
      "|       Working on Chunk 126 / 149!\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-02-01 have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 2 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m previous \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[1;32m---> 34\u001b[0m     question \u001b[38;5;241m=\u001b[39m helper\u001b[38;5;241m.\u001b[39mquestion_gpt(txt, previous)\n\u001b[0;32m     35\u001b[0m     previous\u001b[38;5;241m.\u001b[39mappend(question)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(m):\n",
      "File \u001b[1;32mc:\\Users\\clkds12\\workspace\\domain-embedding\\notebooks\\GPTcalls.py:37\u001b[0m, in \u001b[0;36mGPTAssistant.question_gpt\u001b[1;34m(self, context, previous)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquestion_gpt\u001b[39m(\u001b[38;5;28mself\u001b[39m,context,previous):\n\u001b[1;32m---> 37\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment,\n\u001b[0;32m     38\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     39\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124m            You operate as an assistant to create questions. Use this \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to create a direct question.\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124m            Follow the following instructions:\u001b[39m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124m            1. Ensure each question is clear, concise and directly related to the content provided.\u001b[39m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124m            2. Create just one English question.\u001b[39m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124m            3. Avoid questions about tables, figures and quotations.\u001b[39m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124m            4. Create a different question than these: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprevious\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124m            5. Use only the information described in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m},\n\u001b[0;32m     48\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse this text \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, generate a question that explores a specific aspect of the text provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m}])\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32mc:\\Users\\clkds12\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\_utils\\_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\clkds12\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:643\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    611\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    641\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    642\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    644\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    645\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    646\u001b[0m             {\n\u001b[0;32m    647\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    648\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    649\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    650\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    651\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    652\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    656\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    657\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    658\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    659\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    660\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    661\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    662\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    663\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    664\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    665\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    666\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    667\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    668\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    669\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    670\u001b[0m             },\n\u001b[0;32m    671\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    672\u001b[0m         ),\n\u001b[0;32m    673\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    674\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    675\u001b[0m         ),\n\u001b[0;32m    676\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    677\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    678\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    679\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\clkds12\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1266\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1254\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1261\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1262\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1263\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1264\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1265\u001b[0m     )\n\u001b[1;32m-> 1266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mc:\\Users\\clkds12\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:942\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    934\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    935\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    940\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    941\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    943\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    944\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    945\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    946\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    947\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[0;32m    948\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\clkds12\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1031\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1030\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1032\u001b[0m         input_options,\n\u001b[0;32m   1033\u001b[0m         cast_to,\n\u001b[0;32m   1034\u001b[0m         retries,\n\u001b[0;32m   1035\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1036\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1037\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1038\u001b[0m     )\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\clkds12\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1079\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1079\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1080\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1081\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1082\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[0;32m   1083\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1084\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1085\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\clkds12\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1031\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1030\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1032\u001b[0m         input_options,\n\u001b[0;32m   1033\u001b[0m         cast_to,\n\u001b[0;32m   1034\u001b[0m         retries,\n\u001b[0;32m   1035\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1036\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1037\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1038\u001b[0m     )\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\clkds12\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1079\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1079\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1080\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1081\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1082\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[0;32m   1083\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1084\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1085\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\clkds12\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1046\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1043\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1045\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1046\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1049\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1050\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1054\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-02-01 have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 2 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}"
     ]
    }
   ],
   "source": [
    "from GPTcalls import GPTAssistant\n",
    "\n",
    "model = os.environ.get(\"OPENAI_DEPLOY\")\n",
    "helper = GPTAssistant(deployment = model)\n",
    "\n",
    "full_text = []\n",
    "\n",
    "cont = 1\n",
    "for i in docs:\n",
    "    print(f\"Progress: {cont/len(docs)*100:.2f} %!\")\n",
    "    previous = docs[i]['previous']\n",
    "    actual = docs[i]['actual']\n",
    "    next = docs[i]['next']\n",
    "    cont += 1\n",
    "\n",
    "    full_text.append(helper.aux_processing(previous, next, actual))\n",
    "\n",
    "n = 1\n",
    "m = 1\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "instructions = []\n",
    "\n",
    "instruction = helper.create_instruction(full_text[0])\n",
    "\n",
    "print(f'============= STARTED! =============')\n",
    "\n",
    "for i in range(len(full_text)):\n",
    "    print(f'|       Working on Chunk {i} / {len(full_text)}!')\n",
    "    txt = full_text[i]\n",
    "    previous = []\n",
    "    for i in range(n):\n",
    "        question = helper.question_gpt(txt, previous)\n",
    "        previous.append(question)\n",
    "\n",
    "        for j in range(m):\n",
    "            questions.append(question)\n",
    "            answers.append(helper.answer_gpt(txt, question))\n",
    "            instructions.append(instruction)\n",
    "    \n",
    "print(f'============ COMPLETED! ============')\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the chunks\n",
    "import json\n",
    "formatted_text = [{\"Part {:02d}\".format(i + 1): item} for i, item in enumerate(full_text)]\n",
    "\n",
    "output_name = 'chunks.json'\n",
    "with open(output_name, 'w', encoding='utf-8') as f:\n",
    "    json.dump(formatted_text, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_text = []\n",
    "for i in range(len(questions)):\n",
    "    formatted_text.append(\n",
    "        {\n",
    "            \"id\":i,\n",
    "            \"chunk\": full_text[i],\n",
    "            \"question\": questions[i]\n",
    "        }\n",
    "    )\n",
    "output_name = 'questions.json'\n",
    "with open(output_name, 'w', encoding='utf-8') as f:\n",
    "    json.dump(formatted_text, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "chunk_with_id = [str(i)+\" \"+c for i,c in enumerate(full_text)]\n",
    "db = FAISS.from_texts(chunk_with_id, embeddings)\n",
    "print(db.index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "12 A pump is defined as a device that moves a liquid by increasing the energy level of the liquid. There are various types of pumps, including positive displacement pumps and dynamic pumps. Positive displacement pumps operate by displacing a fixed volume in a confined area, such as those utilizing rotary blades like screw, gear, and reciprocating pumps. They provide constant volume delivery, variable differential head, and are relatively insensitive to liquid properties. On the other hand, dynamic pumps, like centrifugal and axial pumps, generate pressure by using rotating impellers and are sensitive to changes in the system and liquid properties.\n",
      "\n",
      "Regardless of whether using positive displacement or dynamic pumps, each pump consists of hydraulic and mechanical components. The hydraulic end involves moving the liquid, while the mechanical end includes components like shafts, bearings, seals, couplings, and casings. The performance relationships of head, horsepower, and efficiency remain the same for all pumps. These relationships are identical across pump types; the efficiency may vary, but the mechanical components function similarly.\n",
      "\n",
      "Positive displacement pumps maintain a constant flow with variable head capacity. For instance, a double-acting piston pump operates by increasing liquid pressure as the piston moves, displacing the liquid irrespective of its specific gravity or viscosity, given sufficient power from the pump driver. In petrochemical plants, refineries, and gas plants, positive displacement pumps like plunger, piston, and reciprocating pumps are commonly found for various applications within specified horsepower limits.\n",
      "\n",
      "Reciprocating pumps, a subset of positive displacement pumps, increase liquid energy through a pulsating action. They include power pumps, direct acting steam pumps, diaphragm pumps, and metering pumps. Pulsations produced by reciprocating pumps can potentially damage the pumps or process system if not properly managed. Anti-pulsation devices are often required to mitigate these pulsations, ensuring efficient and safe operation.\n"
     ]
    }
   ],
   "source": [
    "query = \"How do positive displacement pumps and dynamic pumps differ in terms of their sensitivity to changes in liquid properties and system conditions?\"\n",
    "docs = db.similarity_search(query,5)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.72', '0.87', '0.93', '0.93', '0.94', '0.95', '0.97', '0.98', '0.98', '0.98']\n"
     ]
    }
   ],
   "source": [
    "num_k = 10\n",
    "\n",
    "input_name = 'questions.json'\n",
    "with open(input_name, 'r', encoding='utf-8') as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "num_correct=[0]*num_k\n",
    "\n",
    "for chunk in chunks:\n",
    "    query = chunk[\"question\"]\n",
    "    docs = db.similarity_search(query,num_k)\n",
    "    correct = False\n",
    "    for i, doc in enumerate(docs):\n",
    "        if correct:\n",
    "            num_correct[i]+=1\n",
    "        else:\n",
    "            db_index = int(doc.page_content.split()[0])\n",
    "            if db_index == chunk[\"id\"]:\n",
    "                correct = True\n",
    "                num_correct[i]+=1\n",
    "\n",
    "formatted_array = [\"{:.2f}\".format(num/len(chunks)) for num in num_correct]\n",
    "print(formatted_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
